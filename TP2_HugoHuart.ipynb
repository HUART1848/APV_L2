{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b13c57c2",
   "metadata": {},
   "source": [
    "# HEIG-Vd - Apprentissage supervisé (APV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00163928",
   "metadata": {},
   "source": [
    "## Travail Pratique 02 - Régression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bab2ca",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Informations générales </b>\n",
    "</div>\n",
    "\n",
    "**Professeur**: Stephan Robert\n",
    "\n",
    "**Assistant(s)**: Félicien Hêche\n",
    "\n",
    "**Contact**: stephan.o.robert@gmail.com, felicien.heche@gmail.com ou via Teams\n",
    "\n",
    "**Rendu**: \n",
    "\n",
    "- Date: 18 ocrobre 2023 \n",
    "- Modalité: Travail individuel. Une fois complétées, copiez les réponses dans le ficher \"**TP2_APV_S1-2023_Nom_Prenom.*****\" (*** = ce que vous voulez qui soit lisible pour nous: .pdf, .html, ...) en remplaçant Nom et Prenom par les votres puis uploader votre fichier sur Cyberlearn.\n",
    "- Note: Ce TP est noté sur 6.\n",
    "\n",
    "**Étudiant**:\n",
    "\n",
    "- Prénom Nom\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<b>But du TP </b>\n",
    "</div>\n",
    "\n",
    "Le but de ce deuxième TP est d'étudier la régression logistique à une seule variable. \n",
    "\n",
    "\n",
    "<!---\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Rappel </b>\n",
    "</div>\n",
    "-->\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Table des matières </b>\n",
    "</div>\n",
    "\n",
    "1. Régression logistique\n",
    ">* Visualisation des données\n",
    ">* Implémentation\n",
    ">* Loss function et gradient\n",
    ">* Optimisation des paramètres\n",
    ">* Evaluation de la régression logistique\n",
    "\n",
    "2. Régularisation de la fonction logistique\n",
    ">* Data visualisation\n",
    ">* Kernel trick\n",
    ">* Loss function et gradient\n",
    ">* Seuil de décision\n",
    "\n",
    "3. Régression polynomiale\n",
    ">* Visualisation du dataset\n",
    ">* Régression polynomiale\n",
    ">* Evaluation de notre modèle\n",
    ">* Under vs Overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545e9942",
   "metadata": {},
   "source": [
    "## 1. Régression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad51eb07",
   "metadata": {},
   "source": [
    "Pour ce premier exercice, nous allons construire une régression logistique afin de déterminer si un étudiant doit être accepté dans une université ou non. Pour décider de cela, nous avons à notre disposition les résultats de chaque étudiant à deux examens. Nous possédons également, des données historiques des étudiants des autres années. Plus précisément, nous connaissons leur score à ce même examens ainsi que la décision d'admissions. Ces informations sont contenues dans un dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26973e8f",
   "metadata": {},
   "source": [
    "Nous allons donc construire un classificateur qui estime la probabilité d'admission, basé sur le score aux deux examens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e7f990",
   "metadata": {},
   "source": [
    "### 1.1 Visualisation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6625ec54",
   "metadata": {},
   "source": [
    "Comme d'habitude, nous allons commencer par essayer de comprendre un minimum les données avec lesquelles nous allons travailler. Pour faire cela, nous allons donc commencer par visualiser ces données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0876e86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc, cm\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "008d3fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/ex2data1.txt') as f1:\n",
    "    dataset_1 = np.loadtxt(f1, delimiter = ',',dtype = 'float', usecols = None)\n",
    "\n",
    "X = dataset_1[:, :-1]\n",
    "Y = dataset_1[:, 2]\n",
    "KO = np.where(Y == 0)[0]\n",
    "OK = np.where(Y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ab4d561-768e-4e4f-ba9c-c6da17cba189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[34.62365962, 78.02469282],\n",
       "       [30.28671077, 43.89499752],\n",
       "       [35.84740877, 72.90219803],\n",
       "       [60.18259939, 86.3085521 ],\n",
       "       [79.03273605, 75.34437644],\n",
       "       [45.08327748, 56.31637178],\n",
       "       [61.10666454, 96.51142588],\n",
       "       [75.02474557, 46.55401354],\n",
       "       [76.0987867 , 87.42056972],\n",
       "       [84.43281996, 43.53339331],\n",
       "       [95.86155507, 38.22527806],\n",
       "       [75.01365839, 30.60326323],\n",
       "       [82.30705337, 76.4819633 ],\n",
       "       [69.36458876, 97.71869196],\n",
       "       [39.53833914, 76.03681085],\n",
       "       [53.97105215, 89.20735014],\n",
       "       [69.07014406, 52.74046973],\n",
       "       [67.94685548, 46.67857411],\n",
       "       [70.66150955, 92.92713789],\n",
       "       [76.97878373, 47.57596365],\n",
       "       [67.37202755, 42.83843832],\n",
       "       [89.67677575, 65.79936593],\n",
       "       [50.53478829, 48.85581153],\n",
       "       [34.21206098, 44.2095286 ],\n",
       "       [77.92409145, 68.97235999],\n",
       "       [62.27101367, 69.95445795],\n",
       "       [80.19018075, 44.82162893],\n",
       "       [93.1143888 , 38.80067034],\n",
       "       [61.83020602, 50.25610789],\n",
       "       [38.7858038 , 64.99568096],\n",
       "       [61.37928945, 72.80788731],\n",
       "       [85.40451939, 57.05198398],\n",
       "       [52.10797973, 63.12762377],\n",
       "       [52.04540477, 69.43286012],\n",
       "       [40.23689374, 71.16774802],\n",
       "       [54.63510555, 52.21388588],\n",
       "       [33.91550011, 98.86943574],\n",
       "       [64.17698887, 80.90806059],\n",
       "       [74.78925296, 41.57341523],\n",
       "       [34.18364003, 75.23772034],\n",
       "       [83.90239366, 56.30804622],\n",
       "       [51.54772027, 46.85629026],\n",
       "       [94.44336777, 65.56892161],\n",
       "       [82.36875376, 40.61825516],\n",
       "       [51.04775177, 45.82270146],\n",
       "       [62.22267576, 52.06099195],\n",
       "       [77.19303493, 70.4582    ],\n",
       "       [97.77159928, 86.72782233],\n",
       "       [62.0730638 , 96.76882412],\n",
       "       [91.5649745 , 88.69629255],\n",
       "       [79.94481794, 74.16311935],\n",
       "       [99.27252693, 60.999031  ],\n",
       "       [90.54671411, 43.39060181],\n",
       "       [34.52451385, 60.39634246],\n",
       "       [50.28649612, 49.80453881],\n",
       "       [49.58667722, 59.80895099],\n",
       "       [97.64563396, 68.86157272],\n",
       "       [32.57720017, 95.59854761],\n",
       "       [74.24869137, 69.82457123],\n",
       "       [71.79646206, 78.45356225],\n",
       "       [75.39561147, 85.75993667],\n",
       "       [35.28611282, 47.02051395],\n",
       "       [56.2538175 , 39.26147251],\n",
       "       [30.05882245, 49.59297387],\n",
       "       [44.66826172, 66.45008615],\n",
       "       [66.56089447, 41.09209808],\n",
       "       [40.45755098, 97.53518549],\n",
       "       [49.07256322, 51.88321182],\n",
       "       [80.27957401, 92.11606081],\n",
       "       [66.74671857, 60.99139403],\n",
       "       [32.72283304, 43.30717306],\n",
       "       [64.03932042, 78.03168802],\n",
       "       [72.34649423, 96.22759297],\n",
       "       [60.45788574, 73.0949981 ],\n",
       "       [58.84095622, 75.85844831],\n",
       "       [99.8278578 , 72.36925193],\n",
       "       [47.26426911, 88.475865  ],\n",
       "       [50.4581598 , 75.80985953],\n",
       "       [60.45555629, 42.50840944],\n",
       "       [82.22666158, 42.71987854],\n",
       "       [88.91389642, 69.8037889 ],\n",
       "       [94.83450672, 45.6943068 ],\n",
       "       [67.31925747, 66.58935318],\n",
       "       [57.23870632, 59.51428198],\n",
       "       [80.366756  , 90.9601479 ],\n",
       "       [68.46852179, 85.5943071 ],\n",
       "       [42.07545454, 78.844786  ],\n",
       "       [75.47770201, 90.424539  ],\n",
       "       [78.63542435, 96.64742717],\n",
       "       [52.34800399, 60.76950526],\n",
       "       [94.09433113, 77.15910509],\n",
       "       [90.44855097, 87.50879176],\n",
       "       [55.48216114, 35.57070347],\n",
       "       [74.49269242, 84.84513685],\n",
       "       [89.84580671, 45.35828361],\n",
       "       [83.48916274, 48.3802858 ],\n",
       "       [42.26170081, 87.10385094],\n",
       "       [99.31500881, 68.77540947],\n",
       "       [55.34001756, 64.93193801],\n",
       "       [74.775893  , 89.5298129 ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ad170bd-ddcc-4550-8a4a-cdd846c976e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_scatter(x, y, figax=None, color=\"red\", edgecolor=None, marker=\"x\", xlab=\"\", ylab=\"\", legend=\"\"):\n",
    "    fig, ax = figax if figax is not None else plt.subplots()\n",
    "    ax.scatter(x, y, color=color, edgecolor=edgecolor, marker=marker, label=legend)\n",
    "    ax.set_xlabel(xlab)\n",
    "    ax.set_ylabel(ylab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c5b0863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f163a7bbf50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "simple_scatter(X[KO][:, 0], X[KO][:, 1], figax=(fig, ax), marker=\"o\", color=\"darkkhaki\", edgecolor=\"black\", legend=\"Not admitted\")\n",
    "simple_scatter(X[OK][:, 0], X[OK][:, 1], figax=(fig, ax), marker=\"+\", color=\"black\", xlab=\"Exam 1 Score\", ylab=\"Exam 2 Score\", legend=\"Admitted\")\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055d4184",
   "metadata": {},
   "source": [
    "## 1.2 Implémentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004272ae",
   "metadata": {},
   "source": [
    "Avant de commencer, rappelons que la regression logistique est définie par\n",
    "$$\n",
    "h(\\Theta) = g(\\Theta^{t}x) = \\frac{1}{1 + e^{-\\Theta^{t}x}}\n",
    "$$\n",
    "où $g$ est la sigmoid\n",
    "$$\n",
    "g(z) := \\frac{1}{1 + e^{-z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148494f1-6cad-4491-801e-1b977484b150",
   "metadata": {},
   "source": [
    "Nous allons maintenant implémenter ces fonctions, ce qui va être utile pour le reste du TP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3de0e214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0b1b97da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_fct(theta, x):\n",
    "    return sigmoid(np.inner(theta, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2552c09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7200759760208356e-44"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the sigmoid function\n",
    "sigmoid(-100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8bdc69",
   "metadata": {},
   "source": [
    "## 1.3 Loss function et gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4f59bf",
   "metadata": {},
   "source": [
    "Nous allons maintenant implémenter la loss fonction, rappelons que cette dernière est\n",
    "$$\n",
    "J(\\Theta) = -\\frac{1}{m} \\left( \\sum_{i=0}^{m}y_{i}\\log(h(x_{i})) + (1 - y_{i})\\log(1 - h(x_{i}))     \\right)\n",
    "$$\n",
    "Et le gradient de cette fonction est un vecteur de même longueur que $\\Theta$ avec\n",
    "$$\n",
    "\\frac{\\partial J(\\Theta)}{\\partial \\theta_{0}} = \\frac{1}{m} \\sum_{i=1}^{m}(h(x_{i}) - y_{i})\n",
    "$$\n",
    "et pour $j>0$, on obtient.\n",
    "$$\n",
    "\\frac{\\partial J(\\Theta)}{\\partial \\theta_{j}} = \\frac{1}{m} \\sum_{i=1}^{m}(h(x_{i}) - y_{i})(x_{i})_{j}\n",
    "$$\n",
    "Remarquez que le gradient ressemble beaucoup au gradient de la régression linéaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf7c36bc-5541-49da-b406-dd5ce2b2691b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.0 * np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a4bf4756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFunction (theta, x, y):\n",
    "    m = len(x)\n",
    "    ret = 0\n",
    "\n",
    "    h = h_fct(theta, x)\n",
    "    return (-1 / m) * np.sum(np.log(h) + (1.0 - y) * np.log(1.0 - h))\n",
    "    #return (-1 / m) * np.sum(np.log(h_fct(theta, x)) + (1 - y) * np.log(1 - h_fct(theta, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cfe938ee-cd98-4de7-8ab5-879deed62b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6752/3311232510.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  return (-1 / m) * np.sum(np.log(h) + (1.0 - y) * np.log(1.0 - h))\n",
      "/tmp/ipykernel_6752/3311232510.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  return (-1 / m) * np.sum(np.log(h) + (1.0 - y) * np.log(1.0 - h))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossFunction(np.array([1.0, 1.0, 2.0]), newX, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d59c17",
   "metadata": {},
   "source": [
    "## 1.4 Optimisation des paramètres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be521e02",
   "metadata": {},
   "source": [
    "Dans le TP précédent, nous avons trouvé les paramètres optimaux de la régression linéaire en utilisant notre propre implémentation de la descente du gradient. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2384a690",
   "metadata": {},
   "source": [
    "En pratique, il est plus commun d'utiliser des packages spécialement conçu pour ce genre de problème. Dans ce TP, nous allons utiliser $\\texttt{scipy.optimize.minimize}$ pour déterminer $\\Theta$. \n",
    "Si la fonction coût a été correctement implémentée $\\texttt{scipy.optimize.minimize}$ va converger vers les paramètres optimaux $\\Theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dea90ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ret is  0\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -0.6931471805599453\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -1.3862943611198906\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -2.0794415416798357\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -2.772588722239781\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -3.4657359027997265\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -4.1588830833596715\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -4.852030263919617\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -5.545177444479562\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -6.238324625039508\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -6.931471805599453\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -7.6246189861593985\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -8.317766166719343\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -9.010913347279288\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -9.704060527839234\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -10.39720770839918\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -11.090354888959125\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -11.78350206951907\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -12.476649250079015\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -13.16979643063896\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -13.862943611198906\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -14.556090791758852\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -15.249237972318797\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -15.942385152878742\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -16.635532333438686\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -17.32867951399863\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -18.021826694558573\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -18.714973875118517\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -19.40812105567846\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -20.101268236238404\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -20.794415416798348\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -21.48756259735829\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -22.180709777918235\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -22.87385695847818\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -23.567004139038122\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -24.260151319598066\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -24.95329850015801\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -25.646445680717953\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -26.339592861277897\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -27.03274004183784\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -27.725887222397784\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -28.419034402957728\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -29.11218158351767\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -29.805328764077615\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -30.49847594463756\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -31.191623125197502\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -31.884770305757446\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -32.57791748631739\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -33.271064666877336\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -33.964211847437284\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -34.65735902799723\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -35.35050620855718\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -36.043653389117125\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -36.73680056967707\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -37.42994775023702\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -38.12309493079697\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -38.816242111356914\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -39.50938929191686\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -40.20253647247681\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -40.895683653036755\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -41.5888308335967\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -42.28197801415665\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -42.9751251947166\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -43.668272375276544\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -44.36141955583649\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -45.05456673639644\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -45.747713916956386\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -46.44086109751633\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -47.13400827807628\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -47.82715545863623\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -48.520302639196174\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -49.21344981975612\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -49.90659700031607\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -50.599744180876016\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -51.29289136143596\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -51.98603854199591\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -52.67918572255586\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -53.372332903115804\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -54.06548008367575\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -54.7586272642357\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -55.451774444795646\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -56.14492162535559\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -56.83806880591554\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -57.53121598647549\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -58.224363167035435\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -58.91751034759538\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -59.61065752815533\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -60.303804708715276\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -60.99695188927522\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -61.69009906983517\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -62.38324625039512\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -63.076393430955065\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -63.76954061151501\n",
      "h_fct 0.5\n",
      "y[i] 0.0\n",
      "ret is  -64.46268779207496\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -65.1558349726349\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -65.84898215319485\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -66.5421293337548\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -67.23527651431475\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -67.9284236948747\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n",
      "ret is  -68.62157087543464\n",
      "h_fct 0.5\n",
      "y[i] 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.693147180559946"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: add a '1' to the first component of each data x\n",
    "# Each data point (x_{1}, x_{2}) should become (1, x_{1}, x_{2})\n",
    "# initial theta values\n",
    "theta = (0, 0, 0)\n",
    "newX = np.concatenate((np.ones((100, 1)), X), axis=1)\n",
    "\n",
    "lossFunction(theta, newX, Y)\n",
    "# apply optimization\n",
    "# TODO: compute the optimal parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fa2ff8",
   "metadata": {},
   "source": [
    "On peut maintenant utiliser ce paramètres $\\Theta$ pour tracer un seuil de jugement sur le training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4607417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot the data and the decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babde9db",
   "metadata": {},
   "source": [
    "### 1.5 Evaluation de la régression logisitique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d52370",
   "metadata": {},
   "source": [
    "Maintenant que nous connaissons les paramètres optimaux de notre régression logistique, il est possible de l'utiliser afin de savoir pour décider si un étudiant doit être accepté ou pas. Pour un étudiant avec un score de $45$ à l'examen $1$ et $85$ à l'examen $2$. Nous devrions trouver un score d'environ $0.7763$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ce276dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO compute the score of a student who has a score of 45 at the first test and 85 at the second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df808d17",
   "metadata": {},
   "source": [
    "## 2. Régularisation de la fonction logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07db2edd",
   "metadata": {},
   "source": [
    "###  2.1 Data visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4db12a6",
   "metadata": {},
   "source": [
    "Pour cet exercice, nous allons utiliser un nouveau dataset. Ce dataset, que vous pouvez trouver dans $\\texttt{ex2data2.txt}$, contient le score de puces éléctroniques à deux différents tests ainsi que si la puce a été jugée conforme à certains critères de qualité ou pas. Plus précisément, $y$ vaut un si la puce a été jugée conforme aux critères de qualités et $y=0$ sinon. Comme avant, commençons par visualiser notre dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30e29ea1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ex2data2.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mex2data2.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f2:\n\u001b[1;32m      2\u001b[0m     dataset_2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mloadtxt(f2, delimiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m,dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m'\u001b[39m, usecols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m X \u001b[38;5;241m=\u001b[39m dataset_2[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ex2data2.txt'"
     ]
    }
   ],
   "source": [
    "with open('ex2data2.txt') as f2:\n",
    "    dataset_2 = np.loadtxt(f2, delimiter = ',',dtype = 'float', usecols = None)\n",
    "    \n",
    "X = dataset_2[:, :-1]\n",
    "Y = dataset_2[:, 2]\n",
    "KO = np.where(Y == 0)[0]\n",
    "OK = np.where(Y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad26ca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64cfec1",
   "metadata": {},
   "source": [
    "Remarquer qu'on ne peut pas ne pas séparer les $y=1$ et les $y=0$ par une fonction linéaire. Par conséquent, une application directe de la régression logistique fournira de mauvais résultats sur ce dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5affc7e4",
   "metadata": {},
   "source": [
    "### 2.2 Kernel trick"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7de27ad",
   "metadata": {},
   "source": [
    "Une manière de contourner cette limitation est de plonger nos données initial dans un espace de plus grande dimension où il sera possible de séparer nos points linéairement. C'est ce que nous appelons le 'kernel trick' (astuce du noyau)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8edca9",
   "metadata": {},
   "source": [
    "Pour faire cela, nous allons utiliser une fonction 'mapFeature' défini par\n",
    "$$\n",
    "(x_{1}, x_{2}) \\mapsto \\left(\n",
    "\\begin{array}{c}\n",
    "1 \\\\\n",
    "x_1 \\\\\n",
    "x_2\\\\\n",
    "x_1^2\\\\\n",
    "x_1x_2\\\\\n",
    "x_2^2\\\\\n",
    "x_1^3\\\\\n",
    "\\vdots \\\\\n",
    "x_1x_2^5\\\\\n",
    "x_2^6\n",
    "\\end{array} \n",
    "\\right)\n",
    "$$\n",
    "plus précisément l'image de $(x_{1}, x_{2})$ est composé des monômes de la forme $x_{1}^{i-j}x_{2}^{j}$ avec $i$ allant de $0$ à $6$ et $j$ allant de $0$ à $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61a1700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapFeature(x1, x2):\n",
    "    # TODO: implement this function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0c3a6a",
   "metadata": {},
   "source": [
    "Cette fonction va nous permettre de plonger notre dataset de dimension $2$ dans un espace de dimension $28$. Ensuite, nous allons appliquer une régression logistique linéaire dans cette espace, ce qui nous permettra de construire un seuil de décisions qui semblera plus complexe lorsque nour le projeterons dans notre espace inital à deux dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97140e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_X = mapFeature(X[:, 0], X[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766e0167",
   "metadata": {},
   "source": [
    "### 2.3 Loss function et gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7634dfaa",
   "metadata": {},
   "source": [
    "Cette approche nous permet de construire des modèles plus performants. Mais cela est également plus susceptible de mener à de l'overffiting. Pour éviter cela, nous allons régulariser notre loss fonction. Plus précisément, la loss fonction devient\n",
    "$$\n",
    "J(\\Theta) = -\\frac{1}{m} \\left( \\sum_{i=0}^{m}y_{i}\\log(h(x)_{i}) + (1 - y_{i})\\log(1 - h(x)_{i}) \\right)\n",
    " + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\theta_{j}^{2}   \n",
    "$$\n",
    "Noter que la deuxième somme commence seulement à l'indice $1$. Ce qui implique que nous ne régularisons pas sur le terme $\\theta_{0}$. En particulier, les dérivées partielles deviennent\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{\\Theta})}{\\partial\\theta_0}=\\frac{1}{m}\\sum_{i=1}^{m}(h(x_i)-y_i)(x_i)_{0} \n",
    "$$\n",
    "pour $j=0$ et pour $j > 1$, \n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{\\Theta})}{\\partial \\theta_j}=\\left[\\frac{1}{m}\\sum_{i=1}^{m}\\left(h(\\mathbf{x}_i)-y_i\\right)(x_i)_j{+\\frac{\\lambda}{m}\\theta_j}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f00486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFunctionReg (theta, x, y, lamb):\n",
    "    # TODO: implement this function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5112b100",
   "metadata": {},
   "source": [
    "### 2.5 Seuil de décision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b4e579",
   "metadata": {},
   "source": [
    "Comme pour l'exercice 1, nous allons maintenant utiliser $\\texttt{scipy.optimize.minimize}$ afin de déterminier les paramètres optimaux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d4fcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(28)\n",
    "lamb = 0.5\n",
    "# TODO: compute the optimal parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb550608",
   "metadata": {},
   "source": [
    "Nous allons maintenant tracer le seuil de décision dans l'espace initial à deux dimensions. Pour faire cela, nous pouvons calculer les valeurs sur une grille (après l'avoir plongé dans l'espace à $28$ dimensions) et tracer les valeurs pour lesquelles les prédictions changent de $y=0$ à $y=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efadc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot the decision boundary using the optimal parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049357ed",
   "metadata": {},
   "source": [
    "Répéter maintenant la procédure ci-dessus en changeant la valeur de $\\lambda$. Que pouvez-vous observer?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b25f8f",
   "metadata": {},
   "source": [
    "## 3. Régression polynomiale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a5b364",
   "metadata": {},
   "source": [
    "Dans cette exercice, nous allons construire une régression polynomiale et étudier l'erreur de notre modèle sur le training et le validation dataset en fonction du degré de notre polynôme."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98245ad2",
   "metadata": {},
   "source": [
    "### 3.1 Visualisation du dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2aaa21",
   "metadata": {},
   "source": [
    "Comme d'habitude, nous allons commencer par visualiser nos datasets. Plus précisément, nous avons à dispositions deux différents datasets. \n",
    "1. Un dataset d'entrainement appelé training dataset, que nous allons utiliser pour entraîner notre modèle, qu'on peut trouver dans le fichier $\\texttt{ex3_training.txt}$.\n",
    "2. Un dataset d'évaluation appelé validation dataset, que nous allons utiliser pour évaluer notre modèle et déterminer nos hyperparamètre (le degré du polynôme d'interpolation dans notre cas) et qu'on peut trouver dans le fichier $\\texttt{ex3_validation.txt}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89af4228",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ex3_training.txt') as f2:\n",
    "    train_data = np.loadtxt(f2, delimiter = ' ',dtype = 'float', usecols = None)\n",
    "\n",
    "X_train = train_data[0]\n",
    "Y_train = train_data[1]\n",
    "\n",
    "\n",
    "with open('ex3_validation.txt') as f2:\n",
    "    eval_data = np.loadtxt(f2, delimiter = ' ',dtype = 'float', usecols = None)\n",
    "X_eval =eval_data[0]\n",
    "Y_eval =eval_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60921292",
   "metadata": {},
   "source": [
    "Pour commencer, nous pouvons déjà faire un plot de ces deux datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f53b37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot this data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2645af65",
   "metadata": {},
   "source": [
    "La couleur des points n'est évidemment pas importante, mais veuillez s'il vous plaît être cohérent sur l'ensemble de cet exercice. Gardez la même couleur pour le jeu de données d'entraînement et le jeu de données de validation tout au long de l'exercice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b7bbf1",
   "metadata": {},
   "source": [
    "### 3.2 Régression polynomiale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4a53f0",
   "metadata": {},
   "source": [
    "Nous allons maintenant utiliser nos données d'entraînement pour faire une régression polynomiale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6d0626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_data(degree, X_train, Y_train, show=False):\n",
    "    \"\"\"\n",
    "    degree: degree of the polynome used for the interpolation\n",
    "    X_train, Y_train:  training data\n",
    "    show: if set to true, this function plot the graph of the interpolation\n",
    "    \"\"\"\n",
    "    # TODO: implement this function\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842897b1",
   "metadata": {},
   "source": [
    "On peut ensuite construire notre fonction 'fit_data' pour déterminer notre regression polynomiale ainsi que faire le plot de cette régression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9b6a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ex3_training.txt') as f2:\n",
    "    train_data = np.loadtxt(f2, delimiter = ' ',dtype = 'float', usecols = None)\n",
    "\n",
    "X_train = train_data[0]\n",
    "Y_train = train_data[1]\n",
    "\n",
    "_, _ = fit_data(3, X_train, Y_train, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a80111",
   "metadata": {},
   "source": [
    "### 3.3 Evaluation de notre modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e88e758",
   "metadata": {},
   "source": [
    "Nous allons maintenant construire une fonction qui nous permettra d'évaluer notre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00accffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(parameters, X_eval, Y_eval, show=False):\n",
    "    \"\"\"\n",
    "    Given the parameters of our model (which is expected to be a polynome), \n",
    "    we compute and return the MSE errors of its predictions on X_eval. \n",
    "    If show is set to true, we plot a graph of these predictions\n",
    "    \"\"\"\n",
    "    # TODO: implement this function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9a751a",
   "metadata": {},
   "source": [
    "En utilisant, ces deux dernières fonctions, on peut alors calculer les paramètres optimaux de notre d'une régression de degré 2 et tracer notre régression sur le training et la validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcac33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ex3_training.txt') as f2:\n",
    "    train_data = np.loadtxt(f2, delimiter = ' ',dtype = 'float', usecols = None)\n",
    "\n",
    "X_train = train_data[0]\n",
    "Y_train = train_data[1]\n",
    "\n",
    "parameters, _ = fit_data(2, X_train, Y_train, True)\n",
    "\n",
    "with open('ex3_validation.txt') as f2:\n",
    "    eval_data = np.loadtxt(f2, delimiter = ' ',dtype = 'float', usecols = None)\n",
    "X_eval =eval_data[0]\n",
    "Y_eval =eval_data[1]\n",
    "\n",
    "_ = evaluate_model(parameters, X_eval, Y_eval, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f0e835",
   "metadata": {},
   "source": [
    "### 3.4 Under vs Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45709766",
   "metadata": {},
   "source": [
    "Nous allons maintenant utiliser les deux fonctions construites en $3.2$ et $3.3$ pour évaluer nos modèles en fonction du degré du polynôme d'interpolation. Pour commencer, nous allons déjà implémenter une fonction qui va construire le polynôme d'interpolation et calculer son erreur sur le training et le validation dataset pour des degrés variant entre deg_min et deg_max. Cette fonction, va retourner ces erreurs sous forme de deux vecteurs de dimensions (deg_max - deg_min)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1fbfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(deg_min, deg_max, X_train, Y_train, X_val, Y_val):\n",
    "    \"\"\"\n",
    "    This function build the polynomial interpolation, using (X_train and Y_train) datasets and evaluate it on (X_val, Y_val)\n",
    "    and compute the train and validation errors.\n",
    "    It builds a such interpolation for all degree between deg_min and deg_max. All training and validation errors are stored and \n",
    "    returned.\n",
    "    \"\"\"\n",
    "    # TODO: implement this function\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c1d52b",
   "metadata": {},
   "source": [
    "En utilisant cette fonction, on peut alors tracer la validation et training erreur en fonction du degré du polynôme d'interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2530777",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ex3_training.txt') as f2:\n",
    "    train_data = np.loadtxt(f2, delimiter = ' ',dtype = 'float', usecols = None)\n",
    "\n",
    "X_train = train_data[0]\n",
    "Y_train = train_data[1]\n",
    "min_degree = 0\n",
    "max_degree = 9\n",
    "\n",
    "# TODO: plot the validation and train error according to the degree of our poylnomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f33c1d",
   "metadata": {},
   "source": [
    "Comme on peut le voir ci-dessus, lorsque notre modèle est de degré petit, l'erreur sur le training et le validation dataset est grand. Dans ce cas, nous sommes en présence d'underfitting. Notre modèle n'est pas assez complexe pour s'adapter à la complexité des données avec lesquelles nous travaillons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0949c34a",
   "metadata": {},
   "source": [
    "Lorsque notre polynôme est degré grand, l'erreur sur notre training dataset est très petite mais celle sur le validation dataset explose. Nous sommes en présence d'overfitting. Notre modèle est trop complexe pour nos données. Il apprend des caractéristiques non-pertinentes des données comme du bruit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
